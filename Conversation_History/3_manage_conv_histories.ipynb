{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing enviornment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000258373FE690>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025838444A90>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model = 'gemma2-9b-it' , groq_api_key = groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(max_tokens=45,   ### Max token count of trimmed messages\n",
    "    \n",
    "    # strategy=\"last\",     ###Strategy for trimming.\"first\": Keep the first <= n_count tokens of the messages.\"last\": Keep the last <= n_count tokens of the messages. Default is \"last\".\n",
    "\n",
    "    token_counter=model, ### Function or llm for counting tokens\n",
    "\n",
    "    include_system=True, ### Whether to keep the SystemMessage if there is one at index 0. Should only be specified if strategy=\"last\". Default is False.\n",
    "    \n",
    "    allow_partial=False, ### Whether to split a message if only part of the message can be included. If strategy=\"last\" then the last partial contents of a message are included. If strategy=\"first\" then the first partial contents of a message are included. Default is False.\n",
    "\n",
    "\n",
    "    start_on=\"human\"   ### The message type to start on. Should only be specified if strategy=\"last\". If specified then every message before the first occurrence of this type is ignored\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(...)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage , HumanMessage , AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To run the below command we need to install transformers package either by pip install transformers or mentioning in requirements.txt\n",
    "\n",
    "### As the max_tokens is less we can observe that few human/AI messages from top are ignored \n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next step is to use the trimmer object to pass only few messages using the prompt to the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the structure of the prompt\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system' , 'You are an helpful AI assistant. Answer all question to the best of your ability in {language}'),\n",
    "    MessagesPlaceholder(variable_name='messages')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to your personal information, including your favorite ice cream! \\n\\nWhat's your favorite flavor? ðŸ˜‹\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why chat model is not able to recall the answer to this question is since the max_token is set to 45 it doesnot have this question in its message history.\n",
    "\n",
    "We can try with different question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You asked:  \"whats 2 + 2\" \\n\\n\\nLet me know if you\\'d like to try another one! ðŸ˜Š \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 87, 'total_tokens': 119, 'completion_time': 0.058181818, 'prompt_time': 0.003157387, 'queue_time': 0.08558302300000001, 'total_time': 0.061339205}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0112ce87-94ea-4b31-9820-45bfb18346e7-0', usage_metadata={'input_tokens': 87, 'output_tokens': 32, 'total_tokens': 119})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did I asked?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next step is to wrap the chain in Message History so that we chat model remembers the message history for each session of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chat message history stores a history of the message interactions in a chat.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory \n",
    "\n",
    "\n",
    "###  Abstract base class for storing chat message history.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory  \n",
    "\n",
    "store = {}\n",
    "\n",
    "### function that retrives chat history for a specific session id\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory wraps another Runnable and manages the chat message history for it; it is responsible for reading and updating the chat message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this time we are using a new parameter -> input_messages_key that knows which messages to select for saving in the chat history\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history=get_session_history,input_messages_key='messages')\n",
    "\n",
    "\n",
    "### Creating a new config for new session \n",
    "config_3 = {'configurable' : {'session_id' : 'chat3'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have access to any personal information about you, including your name.  \\n\\nIf you'd like to tell me your name, I'd be happy to use it! ðŸ˜Š\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 85, 'total_tokens': 132, 'completion_time': 0.085454545, 'prompt_time': 0.00226773, 'queue_time': 0.012029749999999999, 'total_time': 0.087722275}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-72eb2d59-d1f7-4c55-aa75-a3b7a403bb34-0', usage_metadata={'input_tokens': 85, 'output_tokens': 47, 'total_tokens': 132})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({'messages' : messages + [HumanMessage(content='what is my name?')], 'language' : 'English'} ,\n",
    "                            config=config_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat model is not able to remember the name as Bob as we provided earlier in messages since the max_token is set to 45. If you increase the limit lets say 90 it would consider entire messages in that list "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
