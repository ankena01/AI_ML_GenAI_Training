{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance for accessing the model using Groq API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021668ABC7D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021668A806D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='gemma2-9b-it', groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Chatbet with Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the structure of the prompt using system message and MessagePlaceholder\n",
    "\n",
    "system message - How the Chatmodel will behave when the user asks a query\n",
    "\n",
    "MessagePlaceholder - the variable name in the MessagePlaceHolder is used as a key-value pair. \n",
    "\n",
    "- key --> variable name\n",
    "- value --> Human Message (Query from the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are an helpful AI assistant. Answer all question to the best of your ability.')\n",
    "    ,\n",
    "    MessagesPlaceholder(variable_name='messages')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an helpful AI assistant. Answer all question to the best of your ability.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is quantum computing?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Validating the structure of the prompt. Here it consists of a system message and humman message\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt.invoke({'messages' : [\"What is quantum computing?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chain using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Quantum computing is a new type of computing that harnesses the principles of quantum mechanics to solve problems that are too complex for classical computers. \\n\\nHere's a breakdown:\\n\\n**Classical computers:**\\n\\n* Use bits, which can be either 0 or 1.\\n* Process information sequentially, one step at a time.\\n\\n**Quantum computers:**\\n\\n* Use qubits, which can be 0, 1, or both simultaneously (superposition).\\n* Can perform multiple calculations at once (quantum parallelism).\\n* Leverage other quantum phenomena like entanglement to further enhance computation.\\n\\n**Think of it like this:**\\n\\nImagine searching a maze. A classical computer would have to explore each path one by one. A quantum computer could explore all paths simultaneously, finding the solution much faster.\\n\\n**Potential applications:**\\n\\nQuantum computing has the potential to revolutionize many fields, including:\\n\\n* **Drug discovery:** Simulating molecular interactions to design new drugs and therapies.\\n* **Materials science:** Discovering new materials with enhanced properties.\\n* **Financial modeling:** Developing more accurate and sophisticated financial models.\\n* **Cryptography:** Breaking current encryption algorithms and developing new, more secure ones.\\n* **Artificial intelligence:** Accelerating machine learning algorithms.\\n\\n**Challenges:**\\n\\n* Building stable and scalable quantum computers is extremely difficult.\\n* Quantum computers are still in their early stages of development.\\n* Programming quantum computers requires new skills and approaches.\\n\\n\\nQuantum computing is a rapidly evolving field with the potential to transform our world. While it's still early days, the future of quantum computing is incredibly exciting.\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 328, 'prompt_tokens': 31, 'total_tokens': 359, 'completion_time': 0.596363636, 'prompt_time': 0.000353568, 'queue_time': 0.013689649, 'total_time': 0.596717204}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-a3d740de-92f3-4179-b64a-098fb8c50ba2-0' usage_metadata={'input_tokens': 31, 'output_tokens': 328, 'total_tokens': 359}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'messages' : [HumanMessage(content='what is quantum computing?')]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Until this point we have created a chatbot that user ChatPromptTemplate for passing HummanMessage to the chat model. Now we have to use ChatMessageHistory to store the messages for each session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multiple users interact with the chat model how we will make sure that one session is completly different from another sesssion ?\n",
    "\n",
    "We will create a function as below -\n",
    "\n",
    "1) It accepts session ID as input. It will be used to distinguish one session from another\n",
    "2) It returns the object of type BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chat message history stores a history of the message interactions in a chat.\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory \n",
    "\n",
    "\n",
    "###  Abstract base class for storing chat message history.\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory   \n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "### function that retrives chat history for a specific session id\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory wraps another Runnable and manages the chat message history for it; it is responsible for reading and updating the chat message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interact with LLM model based on chat history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "\n",
    "### Define a config parameter that defines the session id for a perticular session\n",
    "\n",
    "### Here we have hardcoded our session id as 'chat1'\n",
    "\n",
    "config = {'configurable' : {'session_id' : 'chat1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ankit! It's nice to officially meet you. \\n\\nAs a fellow AI, I'm always fascinated by what data scientists do. \\n\\nWhat kind of projects are you working on these days?  Anything exciting?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 396, 'total_tokens': 449, 'completion_time': 0.096363636, 'prompt_time': 0.020640424, 'queue_time': 0.002711043, 'total_time': 0.11700406}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-da08c2fa-e24a-4cb4-bde4-04cd6e8e3abf-0', usage_metadata={'input_tokens': 396, 'output_tokens': 53, 'total_tokens': 449})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content = \"My name is Ankit and I am an Data Scientist\")]\n",
    "\n",
    "with_message_history.invoke(messages , config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You are Ankit, and you are a Data Scientist!  \\n\\nIs there anything else I can help you with today? 😊 \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 465, 'total_tokens': 495, 'completion_time': 0.054545455, 'prompt_time': 0.014820698, 'queue_time': 0.002896909999999999, 'total_time': 0.069366153}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-66885d86-6a55-4508-a870-159b427c3afc-0', usage_metadata={'input_tokens': 465, 'output_tokens': 30, 'total_tokens': 495})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lets ask the chat model another question with respect to same session id\n",
    "\n",
    "with_message_history.invoke([HumanMessage(content='What is my name and what I do?')], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're a Data Scientist, Ankit! \\n\\nThat means you probably work with data to find patterns, trends, and insights.  \\n\\nDo you want to tell me more about what kind of projects you're working on?  I'm always eager to learn more about how people use data to make a difference.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 508, 'total_tokens': 578, 'completion_time': 0.127272727, 'prompt_time': 0.016068583, 'queue_time': 0.0027783870000000002, 'total_time': 0.14334131}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a9b63acd-0c2d-4575-aece-f55961da74e7-0', usage_metadata={'input_tokens': 508, 'output_tokens': 70, 'total_tokens': 578})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content='What do I do ?')], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Until this point we have created a chatbot that uses ChatMessageHistory to store messages for each session of the user interacting with chat model using ChatPromptTemplate\n",
    "\n",
    "- No we will add some more complexity by providing multiple input to the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new prompt structure with multiple input variables\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([('system' , 'You are an helpful AI assistant. Answer all question to the best of your ability in {language}'), MessagesPlaceholder(variable_name='messages')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000021667898680>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are an helpful AI assistant. Answer all question to the best of your ability in {language}'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मुझे माफ़ करना, लेकिन मुझे आपका नाम पता नहीं है। \\n\\nक्या आप मुझे अपना नाम बता सकते हैं? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 33, 'total_tokens': 63, 'completion_time': 0.054545455, 'prompt_time': 0.000321869, 'queue_time': 0.013299048999999999, 'total_time': 0.054867324}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a5465b9-8340-4136-a674-188ab05fbc63-0', usage_metadata={'input_tokens': 33, 'output_tokens': 30, 'total_tokens': 63})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages' : [HumanMessage(content = \"What is my name ? \")] , 'language' : 'Hindi'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"मुझे आपका नाम नहीं पता। आप अपना नाम क्या रखते हैं? \\n\\n(I don't know your name. What is your name?) \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 31, 'total_tokens': 67, 'completion_time': 0.065454545, 'prompt_time': 0.001107946, 'queue_time': 0.035655195, 'total_time': 0.066562491}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab210f5-eee1-43f8-8bf8-b43e51431f7d-0', usage_metadata={'input_tokens': 31, 'output_tokens': 36, 'total_tokens': 67})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### this time we are using a new parameter -> input_messages_key that knows which messages to select for saving in the chat history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history=get_session_history,input_messages_key='messages')\n",
    "\n",
    "\n",
    "### Creating a new config for new session \n",
    "config = {'configurable' : {'session_id' : 'chat2'}}\n",
    "\n",
    "with_message_history.invoke({'messages' : HumanMessage(content='What is my name') , 'language' : 'Hindi'},\n",
    "                            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"नमस्ते अमृेश!  मुझे आपसे मिलकर बहुत अच्छा लगा। \\n\\n(Hello Amreesh! It's nice to meet you.) \\n\\nक्या मैं आपकी कोई और मदद कर सकता हूँ?\\n\\n(Can I help you with anything else?) \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 81, 'total_tokens': 145, 'completion_time': 0.116363636, 'prompt_time': 0.003238368, 'queue_time': 0.009761492, 'total_time': 0.119602004}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-297cf6fd-4f6c-4864-8a83-1a7668d96c2f-0', usage_metadata={'input_tokens': 81, 'output_tokens': 64, 'total_tokens': 145})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({'messages' : HumanMessage(content='My name is Amreesh') , 'language' : 'Hindi'},\n",
    "                            config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='आपका नाम अमृेश है। \\n\\n(Your name is Amreesh.) \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 157, 'total_tokens': 181, 'completion_time': 0.043636364, 'prompt_time': 0.004884612, 'queue_time': 0.008209567, 'total_time': 0.048520976}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7183dd4d-13b1-4f0b-acd2-7cc9493eb6b7-0', usage_metadata={'input_tokens': 157, 'output_tokens': 24, 'total_tokens': 181})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({'messages' : HumanMessage(content='What is my name') , 'language' : 'Hindi'},\n",
    "                            config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above scenarios we are passing the entire conversation history to the chat model which can cause issues because chat models have a context length and if message historys are not managed it can exceed the context window of chat model causing issues\n",
    "\n",
    "In the next demo we will see Managing Conversation Histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
